# 实验目的

   熟悉Llama 3.2大语言模型的算法原理，掌握在DLP平台上移植优化聊天机器人的方法和流程；能够使用LoRA对Llama3.2模型进行微调；能够使用Triton语言进行Flash Attention算子的封装并对Llama3.2模型进行算子优化，具体包括：

   1）掌握Llama系列模型的基本原理，特别是基于大型语言模型 Llama 3.2 构建聊天机器人的基本概念和操作步骤，深入理解其在对话生成、多模态生成中的应用；

   2）了解轻量化微调技术的关键概念，包括LoRA轻量化微调等基本原理，掌握使用SWIFT轻量化训练推理工具进行LoRA微调的基本方法；

   3）了解Triton语言进行Flash Attention算法实现的基本原理、算法流程；

   4）能够在DLP 平台上部署Llama3.2模型，实现人机快速聊天应用并实现轻量化微调。

   5）能够使用Flash Attention算子对Llama3.2模型进行优化。

   实验工作量：约270行代码，20小时。



# 实验环境

   硬件平台：DLP云平台环境。

   软件环境：编程框架Pytorch1.13.1、CNNL高性能AI运算库，CNRT 运行时库，以及 python 环境及相关的扩展库。

   

# 实验内容和步骤

   详情请查看第八章实验指导书



#  评分标准

• 60分标准：能够基于Llama3正确实现原生Llama3.2模型的文本生成并进行文本推理速度测试。
• 70分标准：在60分标准的基础上，能够实现原生Llama3.2的多模态生成。
• 80分标准：在70分标准基础上，能够基于SWIFT轻量化推理训练工具正确实现基于LoRA的轻量化微调。 
• 90分标准：在80分的基础上，能够正确实现基于寒武纪Triton的Flash Attention算子封装，并能够正确实现单算子测试。 
• 100分标准：在90分的基础上，能够正确完成Llama3.2的算子替换，可以使用Flash Attention算子正确实现Llama3.2的模型功能。

# 文件提交格式

    需要提交的文件为llama3.2目录下的infer-3b.py、infer_speed_test.py、infer-11b.py、finetune_ruozhiba.sh、math.sh、latexocr.sh、infer_ruozhiba.sh、infer_math.sh、infer_latexocr.sh、flash_attention_triton_opt.py文件以及/opt/tools/native/transformers_mlu/src/transformers/models/mllama/modeling_mllama.py文件，将上述文件直接打包为 zip 文件提交。

